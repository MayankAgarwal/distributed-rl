{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import imageio\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dqn import DQN\n",
    "from modules.env import Env\n",
    "from modules.replay_memory import ReplayMemory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "print \"PyTorch version: \", torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "# Refer https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/3\n",
    "# Since the input is fixed size, this flag could be set to True on GPU for faster performance.\n",
    "torch.backends.cudnn.benchmark = use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_ROM = 'roms/breakout.bin'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 100000  # Memory overflow with 10**6 sized replay memory\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NW_UPDATE_FREQ = 10**4\n",
    "GAMMA = 0.99\n",
    "ACTION_REPEAT = 4\n",
    "IMG_RESCALE_SIZE = (84, 84)\n",
    "PREFILL_REPLAY_MEM_STEPS = 50000\n",
    "NOOP_RANGE = (0, 30)\n",
    "\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "FINAL_EPS_FRAME = 10**6\n",
    "\n",
    "LR = 0.00005\n",
    "REG = 0\n",
    "\n",
    "TRAINING_STEPS = 5000000\n",
    "MODEL_SAVE_STEPS = 25000\n",
    "MOVIE_SAVE_STEPS = 25000\n",
    "\n",
    "RESULTS_FOLDER = 'results/double_dqn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = Env(\n",
    "    os.path.abspath(GAME_ROM), IMG_RESCALE_SIZE, NOOP_RANGE, FloatTensor, AGENT_HISTORY_LENGTH, ACTION_REPEAT)\n",
    "\n",
    "ACTIONS = ENV.action_set\n",
    "ACTION_CNT = len(ACTIONS)\n",
    "\n",
    "Transition = namedtuple('Transitions', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "dqn = DQN(AGENT_HISTORY_LENGTH, ACTION_CNT)\n",
    "target_dqn = DQN(AGENT_HISTORY_LENGTH, ACTION_CNT)\n",
    "\n",
    "if use_cuda:\n",
    "    dqn.cuda()\n",
    "    target_dqn.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(dqn.parameters(), lr=LR, weight_decay=REG)\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE, Transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definition\n",
    "\n",
    "g_steps_done = 0\n",
    "g_last_sync = 0\n",
    "g_total_frames = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon():\n",
    "    global g_steps_done, g_total_frames\n",
    "    \n",
    "    if g_steps_done > FINAL_EPS_FRAME:\n",
    "        return EPS_END\n",
    "\n",
    "    eps = EPS_START + (EPS_END - EPS_START)*g_steps_done / FINAL_EPS_FRAME\n",
    "    return eps\n",
    "\n",
    "def select_action(state):\n",
    "    \n",
    "    global g_steps_done\n",
    "    \n",
    "    result = None\n",
    "    rand = random.random()\n",
    "    eps = get_epsilon()\n",
    "    g_steps_done += 1\n",
    "    \n",
    "    if rand > eps:\n",
    "        dqn.eval()  # Switch model to evaluation mode\n",
    "        pred = dqn(Variable(state, volatile=True).type(FloatTensor)).data.max(1)\n",
    "        dqn.train()  # Switch model back to train mode\n",
    "        \n",
    "        pred = pred[1].view(1, 1) # Single state action\n",
    "        idx = int(pred[0].cpu().numpy())\n",
    "        result = idx\n",
    "    else:\n",
    "        result = random.randrange(0, ACTION_CNT)\n",
    "        \n",
    "    return LongTensor([[result]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global g_last_sync\n",
    "    \n",
    "    dqn.zero_grad()\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    if g_last_sync % TARGET_NW_UPDATE_FREQ == 0:\n",
    "        # https://discuss.pytorch.org/t/are-there-any-recommended-methods-to-clone-a-model/483/3\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "        target_dqn.zero_grad()\n",
    "        \n",
    "        for p in target_dqn.parameters():\n",
    "            p.require_grad = False\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None]), volatile=True)\n",
    "    \n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "    \n",
    "    state_action_values = dqn(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    \n",
    "    # Double Q-Learning\n",
    "    actions = dqn(non_final_next_states).max(1)[1]\n",
    "    actions = actions.view(-1, 1)\n",
    "    next_state_values[non_final_mask] = target_dqn(non_final_next_states).gather(1, actions).squeeze()\n",
    "    \n",
    "#     next_state_values[non_final_mask] = target_dqn(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    expected_state_action_values = reward_batch.squeeze() + GAMMA * next_state_values\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in dqn.parameters():\n",
    "        p.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    g_last_sync += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_replay_mem():\n",
    "    \n",
    "    global memory\n",
    "    \n",
    "    done = False\n",
    "    state = ENV.get_state()\n",
    "\n",
    "    for i in xrange(PREFILL_REPLAY_MEM_STEPS):\n",
    "        \n",
    "        if done:\n",
    "            ENV.reset_game()\n",
    "            state = ENV.get_state()\n",
    "            done = False\n",
    "\n",
    "        action_idx = random.randrange(0, ACTION_CNT)\n",
    "        action_val = ACTIONS[action_idx]\n",
    "        action = LongTensor([[action_idx]])\n",
    "\n",
    "        next_state, reward, done = ENV.take_action(action_val)\n",
    "        reward = Tensor([[reward]])\n",
    "        memory.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        if i%10000 == 0:\n",
    "            print \"%d experiences filled\" % i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling experience replay memory with random actions\n",
      "Replay memory initialized. Length = 0 \n",
      "Training starts\n",
      "0 steps trained. Epsilon: 0.999999\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 4.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-72e48fd7ece2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Mayank/Google Drive/687 - RL/Project/code/modules/env.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__add_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__isdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Mayank/Google Drive/687 - RL/Project/code/modules/env.py\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_tensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Mayank/Google Drive/687 - RL/Project/code/modules/preprocess.pyc\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__scale_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_rewards = []\n",
    "total_rewards = 0.0\n",
    "done = False\n",
    "movie_frames = []\n",
    "\n",
    "last_model_save, last_movie_save = 0, 0\n",
    "\n",
    "save_movie = False\n",
    "save_frame = False\n",
    "\n",
    "# TODO : Pre-fill experience replay memory\n",
    "print \"Filling experience replay memory with random actions\"\n",
    "prefill_replay_mem()\n",
    "print \"Replay memory initialized. Length = %d \" % len(memory)\n",
    "\n",
    "ENV.reset_game()\n",
    "state = ENV.get_state()\n",
    "\n",
    "print \"Training starts\"\n",
    "\n",
    "for step_i in xrange(TRAINING_STEPS):\n",
    "    \n",
    "    if done:\n",
    "        done = ENV.reset_game()\n",
    "        state = ENV.get_state()\n",
    "        \n",
    "        print \"Life complete. Reward = %f\" % total_rewards\n",
    "        game_rewards.append(total_rewards)\n",
    "        \n",
    "        if save_movie and save_frame:\n",
    "            imageio.mimsave(RESULTS_FOLDER + 'train-step_%d__eps_%f.gif' % (step_i, get_epsilon()), movie_frames)\n",
    "            save_movie = False\n",
    "            save_frame = False\n",
    "        elif save_movie and not save_frame:\n",
    "            print \"Saving movie\"\n",
    "            save_frame = True\n",
    "        \n",
    "        total_rewards = 0.0\n",
    "        movie_frames = []\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    action = select_action(state)\n",
    "    action_val = ACTIONS[int(action.cpu().numpy()[0, 0])]\n",
    "    \n",
    "    next_state, reward, done = ENV.take_action(action_val)\n",
    "    \n",
    "    total_rewards += reward\n",
    "    reward = Tensor([[reward]])\n",
    "    \n",
    "    if save_frame:\n",
    "        movie_frames.append(np.copy(ENV.get_current_screen()))\n",
    "    \n",
    "    memory.push(state, action, reward, next_state)\n",
    "    \n",
    "    state = next_state\n",
    "    optimize_model()\n",
    "    \n",
    "    if (step_i - last_model_save) >= MODEL_SAVE_STEPS:\n",
    "        torch.save(dqn.state_dict(), RESULTS_FOLDER + 'dbl_dqn-%d.pth' % step_i)\n",
    "#         torch.save(target_dqn.state_dict(), RESULTS_FOLDER + 'target-dbl_dqn-%d.pth' % step_i)\n",
    "        np.save(RESULTS_FOLDER + 'game_rewards', game_rewards)\n",
    "        last_model_save = step_i\n",
    "    \n",
    "    if (step_i - last_movie_save) >= MOVIE_SAVE_STEPS:\n",
    "        save_movie = True\n",
    "        last_movie_save = step_i\n",
    "        \n",
    "    if step_i % 1000 == 0:\n",
    "        print \"%d steps trained. Epsilon: %f\" % (step_i, get_epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
