{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import imageio\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.categoricaldqn import CategoricalDQN\n",
    "from modules.env import Env\n",
    "from modules.replay_memory import ReplayMemory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  0.4.0a0+ed64001\n"
     ]
    }
   ],
   "source": [
    "print \"PyTorch version: \", torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "# Refer https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/3\n",
    "# Since the input is fixed size, this flag could be set to True on GPU for faster performance.\n",
    "torch.backends.cudnn.benchmark = use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_ROM = 'roms/breakout.bin'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 100000  # Memory overflow with 10**6 sized replay memory\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NW_UPDATE_FREQ = 10**4\n",
    "GAMMA = 0.99\n",
    "ACTION_REPEAT = 4\n",
    "IMG_RESCALE_SIZE = (84, 84)\n",
    "PREFILL_REPLAY_MEM_STEPS = 1000#50000\n",
    "NOOP_RANGE = (0, 30)\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "FINAL_EPS_FRAME = 1e6\n",
    "\n",
    "LR = 0.00025\n",
    "REG = 0\n",
    "\n",
    "TRAINING_STEPS = 5000000\n",
    "MODEL_SAVE_STEPS = 25000\n",
    "MOVIE_SAVE_STEPS = 25000\n",
    "\n",
    "RESULTS_FOLDER = 'results/categorical_dqn-breakout/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_min, V_max = -10., 10.\n",
    "N_atoms = 51\n",
    "\n",
    "delta_z = (V_max - V_min)/(N_atoms - 1)\n",
    "support = torch.linspace(V_min, V_max, N_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = Env(\n",
    "    os.path.abspath(GAME_ROM), IMG_RESCALE_SIZE, NOOP_RANGE, FloatTensor, AGENT_HISTORY_LENGTH, ACTION_REPEAT)\n",
    "\n",
    "ACTIONS = ENV.action_set\n",
    "ACTION_CNT = len(ACTIONS)\n",
    "\n",
    "Transition = namedtuple('Transitions', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "dqn = CategoricalDQN(AGENT_HISTORY_LENGTH, N_atoms, ACTION_CNT, is_noisy=False)\n",
    "target_dqn = CategoricalDQN(AGENT_HISTORY_LENGTH, N_atoms, ACTION_CNT, is_noisy=False)\n",
    "\n",
    "if use_cuda:\n",
    "    dqn.cuda()\n",
    "    target_dqn.cuda()\n",
    "\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=LR, weight_decay=REG)\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE, Transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definition\n",
    "\n",
    "g_steps_done = 0\n",
    "g_last_sync = 0\n",
    "g_total_frames = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_values(out_probs):\n",
    "    global support\n",
    "    # out_probs - (N, A, Z)\n",
    "    support_cp = support.unsqueeze(1)  # Make support (Z, 1)\n",
    "    q_values = torch.bmm(out_probs, support_cp.unsqueeze(0).expand(out_probs.size(0), *support_cp.size()).type(FloatTensor))\n",
    "    q_values = q_values.squeeze()\n",
    "    return q_values\n",
    "\n",
    "def get_epsilon():\n",
    "    global g_steps_done, g_total_frames\n",
    "    \n",
    "    if g_steps_done > FINAL_EPS_FRAME:\n",
    "        return EPS_END\n",
    "\n",
    "    eps = EPS_START + (EPS_END - EPS_START)*g_steps_done / FINAL_EPS_FRAME\n",
    "    return eps\n",
    "\n",
    "def select_action(state):\n",
    "    \n",
    "    global g_steps_done\n",
    "    \n",
    "    result = None\n",
    "    rand = random.random()\n",
    "    eps = get_epsilon()\n",
    "    g_steps_done += 1\n",
    "    \n",
    "    if rand >= eps:\n",
    "        dqn.eval()  # Switch model to evaluation mode\n",
    "        probs = dqn(Variable(state, volatile=True)).data  # (Actions x N_atoms)\n",
    "        q_vals = get_Q_values(probs)\n",
    "        pred = q_vals.max(0)   # Single state action selection\n",
    "        dqn.train()  # Switch model back to train mode\n",
    "        \n",
    "        pred = pred[1].view(1, 1) # Single state action\n",
    "        idx = int(pred[0].cpu().numpy())\n",
    "        result = idx\n",
    "    else:\n",
    "        result = random.randrange(0, ACTION_CNT)\n",
    "        \n",
    "    return LongTensor([[result]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global g_last_sync, support, V_min, V_max, delta_z, N_atoms\n",
    "    \n",
    "    dqn.zero_grad()\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Sync target network with the prediction network\n",
    "    if g_last_sync % TARGET_NW_UPDATE_FREQ == 0:\n",
    "        # https://discuss.pytorch.org/t/are-there-any-recommended-methods-to-clone-a-model/483/3\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "        target_dqn.zero_grad()\n",
    "        \n",
    "        for p in target_dqn.parameters():\n",
    "            p.require_grad = False\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))  # 32 sized tensor (0-1 tensor)\n",
    "    next_state_batch = Variable(torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None]), volatile=True)\n",
    "    \n",
    "    state_batch = Variable(torch.cat(batch.state))  # 32 x 4 x 84 x 84\n",
    "    action_batch = Variable(torch.cat(batch.action)) # 32 x 1\n",
    "    reward_batch = torch.cat(batch.reward) # 32 x 1\n",
    "    \n",
    "    p_s = dqn(state_batch)    # 32 x 4 x 51\n",
    "    p_sa = p_s.gather(1, action_batch.unsqueeze(2).expand(BATCH_SIZE, 1, N_atoms))   # 32 x 1 x 51\n",
    "    p_sa = p_sa.squeeze()   # 32 x 51\n",
    "    \n",
    "    # Double Q-learning\n",
    "    s_t1_cnt = next_state_batch.size(0)\n",
    "    p_s1 = dqn(next_state_batch).data  # 32 x 4 x 51\n",
    "    q_s1 = get_Q_values(p_s1)   # 32 x 4\n",
    "    a_opt_s1 = q_s1.max(1)[1].unsqueeze(1)   # 32 x 1\n",
    "    \n",
    "    next_state_probs = target_dqn(next_state_batch).data   # X x 4 x 51\n",
    "    next_state_action_probs = next_state_probs.gather(1, a_opt_s1.unsqueeze(2).expand(a_opt_s1.size(0), 1, N_atoms))  # X x 1 x 51\n",
    "    next_state_action_probs = next_state_action_probs.squeeze()   # X x 51\n",
    "    \n",
    "    p_s1_a = torch.Tensor(BATCH_SIZE, N_atoms).zero_().type(FloatTensor)\n",
    "    p_s1_a[non_final_mask.unsqueeze(1).expand(BATCH_SIZE, N_atoms)] = next_state_action_probs.type(FloatTensor)\n",
    "    \n",
    "    Tz = reward_batch + GAMMA * non_final_mask.unsqueeze(1).float() * support.unsqueeze(0)  # (32, 51)\n",
    "    \n",
    "    Tz.clamp_(min=V_min, max=V_max)  # (32, 51)\n",
    "    \n",
    "    b = (Tz - V_min)/delta_z  # (32, 51)\n",
    "    l, u = b.floor().long(), b.ceil().long()  # (32, 51)\n",
    "    \n",
    "    m = torch.Tensor(BATCH_SIZE, N_atoms)\n",
    "    offset = torch.linspace(0, ((BATCH_SIZE - 1) * N_atoms), BATCH_SIZE).long().unsqueeze(1).expand(BATCH_SIZE, N_atoms).type(LongTensor)\n",
    "    m.view(-1).index_add_(0, (l + offset).view(-1), (p_s1_a * (u.float() - b)).view(-1))\n",
    "    m.view(-1).index_add_(0, (u + offset).view(-1), (p_s1_a * (b - l.float())).view(-1))   # 32 x 51\n",
    "    \n",
    "    loss = -torch.sum(Variable(m, requires_grad=False) * p_sa.log())\n",
    "    \n",
    "    dqn.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in dqn.parameters():\n",
    "        p.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    g_last_sync += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_replay_mem():\n",
    "    \n",
    "    global memory\n",
    "    \n",
    "    done = False\n",
    "    state = ENV.get_state()\n",
    "\n",
    "    for i in xrange(PREFILL_REPLAY_MEM_STEPS):\n",
    "        \n",
    "        if done:\n",
    "            ENV.reset_game()\n",
    "            state = ENV.get_state()\n",
    "            done = False\n",
    "\n",
    "        action_idx = random.randrange(0, ACTION_CNT)\n",
    "        action_val = ACTIONS[action_idx]\n",
    "        action = LongTensor([[action_idx]])\n",
    "\n",
    "        next_state, reward, done = ENV.take_action(action_val)\n",
    "        reward = Tensor([[reward]])\n",
    "        memory.push(state, action, reward, next_state)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling experience replay memory with random actions\n",
      "Replay memory initialized. Length = 1000 \n",
      "Training starts\n",
      "0 steps trained. Epsilon: 0.999999\n",
      "Life complete. Reward = 2.000000\n",
      "Life complete. Reward = 0.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 4.000000\n",
      "Life complete. Reward = 2.000000\n",
      "1000 steps trained. Epsilon: 0.999099\n",
      "Life complete. Reward = 2.000000\n",
      "Life complete. Reward = 4.000000\n",
      "Life complete. Reward = 2.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 0.000000\n",
      "2000 steps trained. Epsilon: 0.998199\n",
      "Life complete. Reward = 0.000000\n",
      "Life complete. Reward = 2.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 0.000000\n",
      "Life complete. Reward = 1.000000\n",
      "Life complete. Reward = 2.000000\n",
      "Life complete. Reward = 2.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-195c9bc46b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_model_save\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mMODEL_SAVE_STEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-65b024b8cf02>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Mayank/anaconda2/envs/pytorch_src/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Mayank/anaconda2/envs/pytorch_src/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_rewards = []\n",
    "total_rewards = 0.0\n",
    "done = False\n",
    "movie_frames = []\n",
    "\n",
    "last_model_save, last_movie_save = 0, 0\n",
    "\n",
    "save_movie = False\n",
    "save_frame = False\n",
    "\n",
    "# TODO : Pre-fill experience replay memory\n",
    "print \"Filling experience replay memory with random actions\"\n",
    "prefill_replay_mem()\n",
    "print \"Replay memory initialized. Length = %d \" % len(memory)\n",
    "\n",
    "ENV.reset_game()\n",
    "state = ENV.get_state()\n",
    "\n",
    "print \"Training starts\"\n",
    "\n",
    "for step_i in xrange(TRAINING_STEPS):\n",
    "    \n",
    "    if done:\n",
    "        done = ENV.reset_game()\n",
    "        state = ENV.get_state()\n",
    "        \n",
    "        print \"Life complete. Reward = %f\" % total_rewards\n",
    "        game_rewards.append(total_rewards)\n",
    "        \n",
    "        if save_movie and save_frame:\n",
    "            imageio.mimsave(RESULTS_FOLDER + 'train-step_%d__eps_%f.gif' % (step_i, get_epsilon()), movie_frames)\n",
    "            save_movie = False\n",
    "            save_frame = False\n",
    "        elif save_movie and not save_frame:\n",
    "            print \"Saving movie\"\n",
    "            save_frame = True\n",
    "        \n",
    "        total_rewards = 0.0\n",
    "        movie_frames = []\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    action = select_action(state)\n",
    "    action_val = ACTIONS[int(action.cpu().numpy()[0, 0])]\n",
    "    \n",
    "    next_state, reward, done = ENV.take_action(action_val)\n",
    "    \n",
    "    total_rewards += reward\n",
    "    reward = Tensor([[reward]])\n",
    "    \n",
    "    if save_frame:\n",
    "        movie_frames.append(np.copy(ENV.get_current_screen()))\n",
    "    \n",
    "    memory.push(state, action, reward, next_state)\n",
    "    \n",
    "    state = next_state\n",
    "    optimize_model()\n",
    "    \n",
    "    if (step_i - last_model_save) >= MODEL_SAVE_STEPS:\n",
    "        torch.save(dqn.state_dict(), RESULTS_FOLDER + 'noisy_dqn-%d.pth' % step_i)\n",
    "        np.save(RESULTS_FOLDER + 'game_rewards', game_rewards)\n",
    "        last_model_save = step_i\n",
    "    \n",
    "    if (step_i - last_movie_save) >= MOVIE_SAVE_STEPS:\n",
    "        save_movie = True\n",
    "        last_movie_save = step_i\n",
    "        \n",
    "    if step_i % 1000 == 0:\n",
    "        print \"%d steps trained. Epsilon: %f\" % (step_i, get_epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [pytorch_src]",
   "language": "python",
   "name": "Python [pytorch_src]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
