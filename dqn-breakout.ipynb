{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from ale_python_interface import ALEInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dqn import DQN\n",
    "from modules.preprocess import Preprocess\n",
    "from modules.replay_memory import ReplayMemory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_ROM = 'roms/breakout.bin'\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.setBool('display_screen', False)\n",
    "ale.loadROM(GAME_ROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 10**6\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NW_UPDATE_FREQ = 10**4\n",
    "GAMMA = 0.99\n",
    "ACTION_REPEAT = 4\n",
    "IMG_RESCALE_SIZE = (84, 84)\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "FINAL_EPS_FRAME = 10**6\n",
    "\n",
    "LR = 0.00025\n",
    "GRADIENT_MOMENTUM = 0.95    # UNUSED\n",
    "SQ_GRADIENT_MOMENTUM = 0.95 # UNUSED\n",
    "\n",
    "ACTIONS = ale.getLegalActionSet()\n",
    "ACTION_CNT = len(ACTIONS)\n",
    "\n",
    "TRAIN_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocess(IMG_RESCALE_SIZE)\n",
    "Transition = namedtuple('Transitions', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "dqn = DQN(AGENT_HISTORY_LENGTH, ACTION_CNT)\n",
    "target_dqn = DQN(AGENT_HISTORY_LENGTH, ACTION_CNT)\n",
    "\n",
    "optimizer = optim.RMSprop(dqn.parameters(), lr=LR)\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE, Transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definition\n",
    "\n",
    "g_steps_done = 0\n",
    "g_last_sync = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon():\n",
    "    global g_steps_done\n",
    "    \n",
    "    if g_steps_done > FINAL_EPS_FRAME:\n",
    "        return EPS_END\n",
    "\n",
    "    eps = EPS_START + (EPS_END - EPS_START)*g_steps_done / FINAL_EPS_FRAME\n",
    "    return eps\n",
    "\n",
    "def select_action(state):\n",
    "    \n",
    "    global g_steps_done\n",
    "    \n",
    "    rand = random.random()\n",
    "    eps = get_epsilon()\n",
    "    g_steps_done += 1\n",
    "    \n",
    "    if rand > eps:\n",
    "        pred = dqn(Variable(state, volatile=True).type(FloatTensor)).data.max(1)\n",
    "        pred = pred[1].view(1, 1) # Single state action\n",
    "        return pred\n",
    "    else:\n",
    "        return LongTensor([[int(random.choice(ACTIONS))]])\n",
    "\n",
    "def get_state_tensor(state):\n",
    "    \n",
    "    state_tensor = torch.from_numpy(state).type(FloatTensor)\n",
    "    if len(state_tensor.size()) == 3:  # 3d tensor\n",
    "        state_tensor = state_tensor.unsqueeze(0)\n",
    "    \n",
    "    return state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global g_last_sync\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    if g_last_sync % TARGET_NW_UPDATE_FREQ == 0:\n",
    "        # https://discuss.pytorch.org/t/are-there-any-recommended-methods-to-clone-a-model/483/3\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "        \n",
    "        for p in target_dqn.parameters():\n",
    "            p.require_grad = False\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None]), volatile=True)\n",
    "    \n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "    \n",
    "    state_action_values = dqn(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = target_dqn(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    g_last_sync += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "\n",
    "for episode in xrange(TRAIN_EPISODES):\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    action_count = 0\n",
    "    \n",
    "    ale.reset_game()\n",
    "    w, h = ale.getScreenDims()\n",
    "    curr_frame = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    ale.getScreenRGB(screen_data=curr_frame)\n",
    "    \n",
    "    # Stores X+1 frames because it needs 0th index to compute the pixel max between 0th and 1st image\n",
    "    last_K_frames = [curr_frame] * (AGENT_HISTORY_LENGTH+1)\n",
    "    state = get_state_tensor(preprocessor.process_images(last_K_frames))\n",
    "    \n",
    "    # Use lives remaining to define an episode\n",
    "    while not done:\n",
    "        # Skip-framing\n",
    "        if action_count == 0:\n",
    "            action = select_action(state)\n",
    "            action_int = int(action.numpy()[0, 0])\n",
    "        \n",
    "        action_count = (action_count + 1)%ACTION_REPEAT\n",
    "        \n",
    "        reward = ale.act(action_int)\n",
    "        reward = max(min(1, reward), -1)  # Clamp rewards in range [-1, 1]\n",
    "        done = (ale.lives()==0)\n",
    "        \n",
    "        total_reward += reward\n",
    "        reward = Tensor([reward])\n",
    "        \n",
    "        if not done:\n",
    "            ale.getScreenRGB(screen_data=curr_frame)\n",
    "            last_K_frames.append(curr_frame)\n",
    "            last_K_frames = last_K_frames[:(AGENT_HISTORY_LENGTH+1)]\n",
    "            next_state = get_state_tensor(preprocessor.process_images(last_K_frames))\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        memory.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        \n",
    "    if episodes % 10 == 0:\n",
    "        torch.save(dqn.state_dict(), 'results/dqn-%d.pth' % episode)\n",
    "        torch.save(target_dqn.state_dict(), 'results/target-dqn-%d' % episode)\n",
    "        np.save('results/expected_rewards', expected_rewards)\n",
    "    \n",
    "    print \"Episode %d, Total Reward: %f\" % (episode, total_reward)\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
